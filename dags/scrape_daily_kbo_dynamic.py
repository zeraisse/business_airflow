from __future__ import annotations

import os
from datetime import datetime, timedelta

from airflow.decorators import dag, task
from airflow.models.variable import Variable
from airflow.exceptions import AirflowFailException
# Code m√©tier du scraper
from scraper.scraper_csv import (
    list_enterprise_numbers_from_csv,
)
# On importe la fonction de t√©l√©chargement depuis le bon module
from scraper.scraper_basic import download_html


# Dans le conteneur, ./data est mont√© sur /opt/airflow/data
AIRFLOW_DATA_PATH = os.environ.get("AIRFLOW_HOME", "/opt/airflow")
CSV_PATH = os.path.join(AIRFLOW_DATA_PATH, "data", "enterprise.csv")

# üëâ On veut traiter jusqu'√† 20 entreprises par run
MAX_COMPANIES_PER_RUN = 20

default_args = {
    "owner": "faycal",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}


@dag(
    dag_id="scrape_daily_kbo_dynamic",
    description="Scraping KBO avec t√¢ches dynamiques (plusieurs entreprises par run, 1 entreprise = 1 task)",
    start_date=datetime(2025, 11, 13),
    # üëâ on ex√©cute le DAG toutes les minutes
    schedule="* * * * *",
    catchup=False,
    default_args=default_args,
    tags=["kbo", "scraping", "dynamic"],
    max_active_runs=1,  # 1 run √† la fois, mais plusieurs tasks en parall√®le dans ce run
)
def scrape_daily_kbo_dynamic_dag():
    """
    DAG dynamique pour scraper les donn√©es KBO.
    1. R√©cup√®re la liste des proxies valides (pr√©par√©e par `proxy_manager_dag`).
    2. Lit `enterprise.csv` pour trouver les entreprises √† scraper.
    3. Cr√©e une t√¢che de scraping par entreprise, en lui assignant un proxy.
    """

    @task(task_id="get_available_proxies")
    def _get_available_proxies() -> list[str]:
        """
        R√©cup√®re la liste des proxies depuis la Variable Airflow `proxy_list`.
        Cette variable est maintenue par le DAG `proxy_manager_dag`.
        """
        # On r√©cup√®re la liste de proxies, qui a d√©j√† √©t√© test√©e et valid√©e.
        proxies_str = Variable.get("proxy_list", default_var=None)
        
        if not proxies_str:
            raise AirflowFailException("La variable 'proxy_list' est vide ou n'existe pas. Ex√©cutez d'abord 'proxy_manager_dag'.")

        all_proxies = [p.strip() for p in proxies_str.split(',')]

        # La logique de quarantaine est maintenant dans le proxy_manager.
        # Ici, on utilise simplement ce qui est disponible.
        if not all_proxies:
            # Si aucun proxy n'est dispo, on fait √©chouer le DAG pour ne pas scraper sans.
            raise AirflowFailException("Aucun proxy disponible, arr√™t du DAG.")

        print(f"{len(all_proxies)} proxies valides r√©cup√©r√©s.")
        return all_proxies

    @task(task_id="read_next_enterprises")
    def _read_next_enterprises(
        max_companies: int = MAX_COMPANIES_PER_RUN,
    ) -> list[str]:
        """
        Lit le CSV, regarde quels HTML existent d√©j√†,
        et renvoie une liste d'entreprises √† scraper.
        """
        # Liste compl√®te des num√©ros
        all_numbers = list_enterprise_numbers_from_csv(
            csv_path=CSV_PATH,
            max_companies=None,  # on lit tout, on filtrera apr√®s
        )

        # Le chemin est relatif √† la racine du projet Airflow dans le conteneur
        html_dir = os.path.join(AIRFLOW_DATA_PATH, "data", "html")
        if not os.path.exists(html_dir):
            os.makedirs(html_dir)

        remaining: list[str] = []
        for number in all_numbers:
            html_path = os.path.join(html_dir, f"{number}.html")
            if not os.path.exists(html_path):
                remaining.append(number)
            if len(remaining) >= max_companies:
                break

        print(f"‚û°Ô∏è {len(remaining)} entreprise(s) √† scraper sur ce run (max={max_companies}).")
        if not remaining:
            print("Toutes les entreprises semblent d√©j√† scrap√©es. ‚úîÔ∏è")
        else:
            print(f"Entreprises s√©lectionn√©es : {remaining}")
        return remaining

    @task(
        task_id="scrape_one_enterprise",
        retries=2, # On peut retenter avec un autre proxy
        # üëâ Pour limiter le nombre de scrapings simultan√©s, cr√©ez un Pool "scraping_pool"
        # dans l'UI Airflow (Admin -> Pools) et d√©commentez la ligne suivante.
        pool="scraping_pool",
    )
    def _scrape_one(number: str, proxy: str):
        """
        T√¢che Airflow pour UNE entreprise.
        Si √ßa plante, seule cette task est en √©chec.
        Un proxy diff√©rent est utilis√© pour chaque tentative.
        """
        try:
            print(f"Scraping entreprise: {number} avec proxy {proxy}")
            download_html(number, proxy=proxy)
        except Exception as e: # On capture une exception plus large
            print(f"√âchec du scraping pour {number} avec proxy {proxy}. Erreur: {e}")
            # La logique de quarantaine est g√©r√©e par le `proxy_manager_dag`.
            # Si un proxy √©choue ici, il sera probablement d√©tect√© comme invalide
            # lors du prochain run du `proxy_manager_dag` et sera retir√© de la liste.
            # On pourrait aussi impl√©menter une quarantaine "en temps r√©el" avec Redis ici.
            raise # Fait √©chouer la t√¢che pour qu'Airflow la retente.

    # 1) On r√©cup√®re la liste des proxies et des entreprises
    proxy_list = _get_available_proxies()
    numbers_list = _read_next_enterprises(max_companies=MAX_COMPANIES_PER_RUN)

    # 2) On pr√©pare les arguments pour le mapping dynamique.
    #    On veut une paire (entreprise, proxy) pour chaque t√¢che.
    #    On s'assure de ne pas cr√©er plus de t√¢ches qu'on a de proxies ou d'entreprises.
    @task
    def map_arguments(numbers, proxies):
        import itertools
        num_tasks = min(len(numbers), len(proxies))
        # On cr√©e un cycle sur la liste des proxies si on a plus d'entreprises que de proxies
        proxy_cycle = itertools.cycle(proxies)
        # On retourne un dictionnaire que .expand_kwargs peut utiliser
        return [{"number": num, "proxy": next(proxy_cycle)} for num in numbers[:num_tasks]]

    # 3) On cr√©e dynamiquement une t√¢che pour chaque paire (entreprise, proxy)
    _scrape_one.expand_kwargs(map_arguments(numbers_list, proxy_list))

scrape_daily_kbo_dynamic_dag()
